{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "x = torch.rand(3)\n",
        "print(x)"
      ],
      "metadata": {
        "id": "QkC3Xay1xc_6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7cbe79c-74e3-46d3-d53f-7c25510a3b48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.0173, 0.4004, 0.5728])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# https://pytorch.org/docs/stable/generated/torch.Tensor.requires_grad.html   # (requires_grad = True)    Documentation\n",
        "\n",
        "x = torch.rand(3, requires_grad=True)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lpZkyqlyQ2q",
        "outputId": "f5272da9-77a8-411a-cb6d-df9bc16dbfa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.6209, 0.4702, 0.2588], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = x+2  # Y has an attribute grad_fn which point to Gradient function , function is Add_Backword which will calculate the Gradient of y  w.r.t x\n",
        "print(y)\n",
        "\n",
        "# Calculates the computational Graph by PyTorch\n",
        "# Pytorch automatically create a Function for us\n",
        "# This function used in the BackPropagation to get the Gradient\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2l3Jsw3yhpE",
        "outputId": "a7950938-2a14-42de-bf42-f799bf37e9ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2.6209, 2.4702, 2.2588], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z  = y*y*2\n",
        "z = z.mean()\n",
        "\n",
        "print(z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9RcbQJ32QMk",
        "outputId": "2b8417df-b515-456e-e274-f3dc86e8d158"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(12.0488, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html\n",
        "\n",
        "z.backward()   #dz/dx  # Calcualtes the Gradient\n",
        "print(x.grad)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdDvKivF20LI",
        "outputId": "a6035159-495b-4edf-ccc6-39269218b79a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3.4945, 3.2936, 3.0117])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# When we Dont Specify the Argument\n",
        "\n",
        "x = torch.rand(3, requires_grad=False)     # requires_grad = False\n",
        "print(x)\n",
        "\n",
        "y = x+2\n",
        "print(y)\n",
        "\n",
        "z  = y*y*2\n",
        "z = z.mean()\n",
        "\n",
        "print(z)\n",
        "\n",
        "# z.backward()   #dz/dx  # Calcualtes the Gradient\n",
        "# print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJAPCk143eZY",
        "outputId": "dbd7de2a-de5a-49c2-c15f-01d52228cc15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.2060, 0.0339, 0.9101])\n",
            "tensor([2.2060, 2.0339, 2.9101])\n",
            "tensor(11.6481)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Produces Error z.backward()\n",
        "\n",
        "x = torch.rand(3, requires_grad=False)\n",
        "print(x)\n",
        "\n",
        "y = x+2\n",
        "print(y)\n",
        "\n",
        "z  = y*y*2\n",
        "z = z.mean()\n",
        "\n",
        "print(z)\n",
        "\n",
        "z.backward()   #dz/dx  # Calcualtes the Gradient\n",
        "# print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "eU5RxpKB4Lp4",
        "outputId": "ab1e7566-960f-4b78-e2c5-8f74185a405b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([9.4147e-01, 7.7683e-04, 1.1425e-01])\n",
            "tensor([2.9415, 2.0008, 2.1142])\n",
            "tensor(11.4169)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-de47655be22a>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m#dz/dx  # Calcualtes the Gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m# print(x.grad)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Jacobian Matrix\n",
        "# J.v = Jacobian matrix with partial Derivative  * Gradient Vector = Final Gradient\n",
        "\n",
        "# https://machinelearningmastery.com/a-gentle-introduction-to-the-jacobian/  # Docuemtation\n",
        "\n",
        "# The Jacobian matrix collects all first-order partial derivatives of a multivariate function that can be used for backpropagation.\n",
        "# The Jacobian determinant is useful in changing between variables, where it acts as a scaling factor between one coordinate space and another\n",
        "\n",
        "# Chain Rule"
      ],
      "metadata": {
        "id": "3JhSpRrZ4Xdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ignoring Mean Operation\n",
        "\n",
        "x = torch.rand(3, requires_grad=True)\n",
        "print(x)\n",
        "\n",
        "y = x+2\n",
        "print(y)\n",
        "\n",
        "z  = y*y*2\n",
        "#z = z.mean()\n",
        "\n",
        "print(z)\n",
        "\n",
        "# z.backward()\n",
        "# print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZjNirl95-49",
        "outputId": "d93b485e-3e50-4e6f-ed63-393fbae56132"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.3277, 0.4392, 0.1337], requires_grad=True)\n",
            "tensor([2.3277, 2.4392, 2.1337], grad_fn=<AddBackward0>)\n",
            "tensor([10.8367, 11.8992,  9.1056], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ignoring Mean Operation and Backword On the Produces Error\n",
        "\n",
        "x = torch.rand(3, requires_grad=True)\n",
        "print(x)\n",
        "\n",
        "y = x+2\n",
        "print(y)\n",
        "\n",
        "z  = y*y*2\n",
        "#z = z.mean()\n",
        "\n",
        "print(z)\n",
        "\n",
        "z.backward()\n",
        "# print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "cMFADknY6Da4",
        "outputId": "d8061b1e-e1a3-4888-c287-71146a095955"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4959, 0.8067, 0.5198], requires_grad=True)\n",
            "tensor([2.4959, 2.8067, 2.5198], grad_fn=<AddBackward0>)\n",
            "tensor([12.4588, 15.7547, 12.6983], grad_fn=<MulBackward0>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "grad can be implicitly created only for scalar outputs",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-891a6c61e9a4>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m# print(x.grad)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_grads_batched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                     raise RuntimeError(\n\u001b[0m\u001b[1;32m    118\u001b[0m                         \u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                     )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ignoring Mean Operation means Scaler value and adding vector\n",
        "\n",
        "x = torch.rand(3, requires_grad=True)\n",
        "print(x)\n",
        "\n",
        "y = x+2\n",
        "print(y)\n",
        "\n",
        "z  = y*y*2\n",
        "#z = z.mean()\n",
        "\n",
        "print(z)\n",
        "\n",
        "v = torch.tensor([0.1, 1.0, 0.001], dtype=torch.float32)\n",
        "z.backward(v)\n",
        "\n",
        "print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Afpru0nW6l19",
        "outputId": "440b0cf6-0356-45bf-c39d-6f8ecada61c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.8268, 0.7506, 0.5405], requires_grad=True)\n",
            "tensor([2.8268, 2.7506, 2.5405], grad_fn=<AddBackward0>)\n",
            "tensor([15.9815, 15.1319, 12.9080], grad_fn=<MulBackward0>)\n",
            "tensor([1.1307e+00, 1.1002e+01, 1.0162e-02])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch Prevent the tracking the History from the computation Graph & also stops the creating the gradient fucntion\n",
        "\n",
        "x = torch.rand(3, requires_grad=True)\n",
        "print(x)\n",
        "\n",
        "# x.requires_grad_(False)\n",
        "# x.detach()\n",
        "# with torch.no_grad():"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKXgohfk611y",
        "outputId": "816ca5c9-0fb8-41d7-d6b9-4114a94affee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.9421, 0.9525, 0.5449], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(3, requires_grad=True)\n",
        "print(x)\n",
        "\n",
        "x.requires_grad_(False)     # Modiified The Variable (Inplace Operation)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zl1SUYZn9nqm",
        "outputId": "8d3cbfc5-6a6d-48fc-df2d-d5701a5f5cb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4204, 0.8054, 0.1919], requires_grad=True)\n",
            "tensor([0.4204, 0.8054, 0.1919])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(3, requires_grad=True)\n",
        "print(x)\n",
        "\n",
        "y = x.detach()\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7Lsx_oU90tt",
        "outputId": "e98209f0-38a7-4382-be30-f755110a242d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.5876, 0.1002, 0.6165], requires_grad=True)\n",
            "tensor([0.5876, 0.1002, 0.6165])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(3, requires_grad=True)\n",
        "print(x)\n",
        "\n",
        "with torch.no_grad():\n",
        "   y = x + 2\n",
        "   print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPpBSYfT99ku",
        "outputId": "e125fc99-b86c-43a1-998e-2efff854bb12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.7551, 0.0907, 0.1614], requires_grad=True)\n",
            "tensor([2.7551, 2.0907, 2.1614])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Example\n",
        "\n",
        "import torch\n",
        "weights = torch.ones(4, requires_grad=True)\n",
        "\n",
        "for epoch in range(1):   # 1 Iteration\n",
        "     model_output = (weights * 3).sum()\n",
        "     model_output.backward()\n",
        "     print(weights.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tq8cutw-NsZ",
        "outputId": "fced1ae9-fe28-4f9b-d7b3-f8c7f7b5fd17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3., 3., 3., 3.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Example\n",
        "\n",
        "import torch\n",
        "weights = torch.ones(4, requires_grad=True)\n",
        "\n",
        "for epoch in range(3):     # 3 Iterations\n",
        "     model_output = (weights * 3).sum()\n",
        "     model_output.backward()\n",
        "     print(weights.grad)\n",
        "\n",
        "# We got incorrect Gradients"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ko1ky0xwE_66",
        "outputId": "7852b567-1f8d-4e5d-d99d-e60dcec44cd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3., 3., 3., 3.])\n",
            "tensor([6., 6., 6., 6.])\n",
            "tensor([9., 9., 9., 9.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Example\n",
        "\n",
        "import torch\n",
        "weights = torch.ones(4, requires_grad=True)\n",
        "\n",
        "for epoch in range(3):     # 3 Iterations\n",
        "     model_output = (weights * 3).sum()\n",
        "     model_output.backward()\n",
        "     print(weights.grad)\n",
        "     weights.grad.zero_()   # Inplace operation\n",
        "\n",
        "# Now We have got correct Gradients"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I53wxC6-FJCB",
        "outputId": "dd883789-226d-48ca-978d-8a7b9abd5628"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimization (No Run Please)\n",
        "\n",
        "# https://stackoverflow.com/questions/53975717/pytorch-connection-between-loss-backward-and-optimizer-step    # Step function Documentation\n",
        "\n",
        "\n",
        "weights = torch.ones(4, requires_grad=True)\n",
        "\n",
        "optimizer = torch.optim.SGD(weights, lr=0.01)\n",
        "\n",
        "# After computing the gradients for all tensors in the model, calling optimizer.step() makes the optimizer iterate over all parameters (tensors) it is supposed to update and use their internally stored grad to update their values.\n",
        "optimizer.step()\n",
        "optimizer.zero_grad()\n"
      ],
      "metadata": {
        "id": "8BQSq1c-GpwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ( No Run Please )\n",
        "\n",
        "weights = torch.ones(4, requires_grad=True)\n",
        "\n",
        "z.backward()\n",
        "weights.grad.zero_()   # Inplace operation\n"
      ],
      "metadata": {
        "id": "WDp2DIU_I83b"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}